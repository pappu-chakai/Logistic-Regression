{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**THEORY**"
      ],
      "metadata": {
        "id": "sWOkVnGUbbtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "ANS:- Logistic Regression and Linear Regression are both\n",
        "      statistical methods used for prediction, but they differ fundamentally in their purpose and the type of data they handle.\n",
        "\n",
        "      **Linear Regression:**\n",
        "      Purpose: Predicts a continuous outcome variable (dependent variable) based on one or more predictor variables (independent variables).  It models the relationship between variables as a linear equation, aiming to find the best-fitting line through the data points.\n",
        "      Output: A continuous value.  For example, predicting house prices, stock prices, or temperature.\n",
        "      Example:Predicting the price of a house based on its size, location, and number of bedrooms.\n",
        "\n",
        "      **Logistic Regression:**\n",
        "      Purpose: Predicts the probability of a categorical outcome (dependent variable).  Typically, this outcome is binary (e.g., yes/no, 0/1, success/failure).  It models the probability using a sigmoid function, which maps any input to a value between 0 and 1.\n",
        "      Output: A probability score between 0 and 1, which can be interpreted as the likelihood of the outcome belonging to a specific category. A threshold (usually 0.5) is then applied to classify the prediction.\n",
        "      Example:Predicting whether a customer will click on an advertisement (click/no click), whether a patient has a disease (disease/no disease), or whether an email is spam (spam/not spam)."
      ],
      "metadata": {
        "id": "7ewzTl2Pbeqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "ANS:- The mathematical equation of Logistic Regression is:\n",
        "\n",
        "      P(Y=1|X) = 1 / (1 + exp(-(β0 + β1X1 + β2X2 + ... + βnXn)))\n",
        "\n",
        "      Where:\n",
        "\n",
        "      P(Y=1|X) is the probability of the dependent variable Y being 1 given the independent variables X.\n",
        "      X1, X2, ... , Xn are the independent variables.\n",
        "      β0, β1, β2, ... , βn are the regression coefficients (parameters) that the model learns during training.  β0 is the intercept.\n",
        "      exp() represents the exponential function (e raised to the power of the expression in the parentheses).\n",
        "\n",
        "      This equation essentially uses a sigmoid function (1 / (1 + exp(-z))) to transform a linear combination of the input features (β0 + β1X1 + ... + βnXn) into a probability between 0 and 1.  The sigmoid function's S-shape is crucial for mapping any input to a value within the probability range.\n",
        "\n",
        "      The model learns the optimal values for the regression coefficients (β's) during the training process by minimizing a cost function (e.g., log-loss).  The goal is to find the coefficients that best fit the relationship between the independent and dependent variables in the training data."
      ],
      "metadata": {
        "id": "USK8Rn05chAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "ANS:- The Sigmoid function is crucial in logistic regression\n",
        "      because it transforms the linear combination of input features into a probability between 0 and 1.  \n",
        "      This is essential for binary classification problems, as the output of logistic regression needs to represent the probability of an instance belonging to a particular class.\n",
        "      The sigmoid function's S-shaped curve maps any input value to a probability within this 0-1 range."
      ],
      "metadata": {
        "id": "mam94DRrdM7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the cost function of Logistic Regression?\n",
        "\n",
        "ANS:- The cost function of Logistic Regression is typically\n",
        "      the Log-Loss function (also known as cross-entropy loss).  \n",
        "      For a single training example:\n",
        "      Cost(hθ(x), y) = -y * log(hθ(x)) - (1 - y) * log(1 - hθ(x))\n",
        "      Where:\n",
        "      hθ(x) is the predicted probability that y = 1 (the output of the sigmoid function).\n",
        "      y is the actual value (0 or 1).\n",
        "\n",
        "      The overall cost function for the entire training set is the average of the costs for all examples:\n",
        "      J(θ) = (1/m) * Σ [ -y(i) * log(hθ(x(i))) - (1 - y(i)) * log(1 - hθ(x(i))) ]\n",
        "      where:\n",
        "      m is the number of training examples.\n",
        "      The sum is taken over all training examples from i = 1 to m.\n",
        "\n",
        "      The goal of training a logistic regression model is to find the parameters (θ) that minimize this cost function J(θ).\n",
        "      This is typically done using optimization algorithms like gradient descent.\n"
      ],
      "metadata": {
        "id": "2acqfK5PdgAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "ANS:- Regularization in logistic regression is a technique\n",
        "      used to prevent overfitting.\n",
        "      Overfitting occurs when the model learns the training data too well, including its noise and outliers, leading to poor performance on unseen data.  \n",
        "      Regularization adds a penalty term to the cost function, discouraging the model from assigning excessively large weights to features.  This helps to simplify the model and make it more generalizable.\n",
        "\n",
        "      Why is it needed?\n",
        "\n",
        "      Prevents Overfitting: By constraining the magnitude of coefficients, regularization reduces the model's complexity, making it less prone to overfitting the training data and improving its ability to generalize to new, unseen data.\n",
        "      Improves Generalization: A regularized model is more likely to perform well on unseen data because it's less sensitive to the specific nuances of the training data.\n",
        "      Handles Multicollinearity:  When features are highly correlated (multicollinearity), regularization can help to stabilize the model's coefficients and prevent them from becoming overly sensitive to small changes in the data. Feature Selection (L1): L1 regularization can automatically select important features by shrinking the coefficients of less important features to zero.  This can lead to a more interpretable and efficient model."
      ],
      "metadata": {
        "id": "gL428fDyd4Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "ANS:- Lasso, Ridge, and Elastic Net are regularization\n",
        "      techniques used in linear regression (and can be adapted for other models) to prevent overfitting. They differ in how they penalize the magnitude of the coefficients:\n",
        "      Lasso (L1 regularization): Adds a penalty term proportional to the absolute values of the coefficients to the cost function. This can shrink some coefficients to exactly zero, effectively performing feature selection.  It's useful when you suspect that only a subset of your features are truly relevant.\n",
        "\n",
        "      Ridge (L2 regularization): Adds a penalty term proportional to the square of the coefficients to the cost function. This shrinks the coefficients towards zero but doesn't typically set them to exactly zero.  It's effective in handling multicollinearity (high correlation between features).\n",
        "\n",
        "      Elastic Net: Combines L1 and L2 regularization. It adds a penalty term that's a linear combination of the L1 and L2 penalties.  This combines the benefits of both Lasso and Ridge.  It's particularly useful when you have a high number of features and some of them are highly correlated."
      ],
      "metadata": {
        "id": "YY_bNH9XeXqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "ANS:- Elastic Net combines the strengths of both Lasso and\n",
        "      Ridge regression.  Use Elastic Net when:\n",
        "\n",
        "      1. **High dimensionality and multicollinearity:**  You\n",
        "         have many features, and some of them are highly correlated.  Lasso might arbitrarily pick one from a group of correlated features, while Ridge shrinks all of them. Elastic Net balances this by performing both feature selection (like Lasso) and coefficient shrinkage (like Ridge), leading to a more stable and robust model.\n",
        "\n",
        "     2. **Groups of correlated predictors:** When you expect\n",
        "        groups of predictors to be relevant, Elastic Net is often preferable. It can select groups of correlated variables, preventing the arbitrary selection that can occur with Lasso.\n",
        "\n",
        "     3. **Uncertainty about the optimal penalty type:** If\n",
        "        you're not sure whether L1 or L2 regularization is more appropriate, Elastic Net provides a flexible way to combine both. The mixing parameter (alpha) allows you to tune the balance between L1 and L2 penalties.\n",
        "\n",
        "     4. **Improved prediction accuracy:**  In some cases,\n",
        "        Elastic Net can lead to better prediction accuracy compared to Lasso or Ridge alone, especially when dealing with highly correlated features or a large number of predictors."
      ],
      "metadata": {
        "id": "bsWYTt1det1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "ANS:- In logistic regression, the regularization parameter\n",
        "      (λ), also known as the penalty parameter or regularization strength, controls the amount of regularization applied to the model.  It directly affects the magnitude of the coefficients (weights) assigned to the features.\n",
        "\n",
        "      Impact of λ:\n",
        "\n",
        "      λ = 0: No regularization. The model can fit the training data very closely, potentially leading to overfitting, especially with a large number of features or complex relationships.  The coefficients might be very large.\n",
        "\n",
        "      0 < λ < ∞:  Regularization is applied. As λ increases, the penalty for large coefficients becomes stronger. This leads to:\n",
        "\n",
        "      Smaller coefficient magnitudes:  The model's coefficients are shrunk towards zero.  Features with less importance will have coefficients closer to zero.\n",
        "      Reduced model complexity: The model becomes simpler and less sensitive to noise in the training data.\n",
        "      Improved generalization:  The model's performance on unseen data typically improves because it's less likely to overfit the training data.\n",
        "      Potential for feature selection (L1 regularization): With L1 regularization (Lasso), increasing λ can shrink some coefficients all the way to zero, effectively performing feature selection.  This is not typically the case with L2 regularization (Ridge).\n",
        "\n",
        "      λ = ∞:  Maximum regularization. All coefficients become zero (or very close to zero). The model becomes a constant predictor and fails to learn any relationship from the data."
      ],
      "metadata": {
        "id": "FiiTnovpfGAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What are the key assumptions of Logistic Regression?\n",
        "\n",
        "ANS:- However, based on the text provided, here are some key\n",
        "      assumptions of logistic regression:\n",
        "\n",
        "     1. Binary Logistic Regression: Assumes a binary outcome\n",
        "        variable.\n",
        "     2. Linearity of independent variables and log-odds:  \n",
        "        Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable.\n",
        "     3. Independence of observations:  The observations\n",
        "        should be independent of each other.\n",
        "     4. No multicollinearity among predictors: High\n",
        "        correlation between independent variables can cause instability in the coefficient estimates.\n",
        "     5. Large sample size:  Logistic Regression performs\n",
        "        better with larger datasets to provide reliable estimates.\n",
        "     6. No outliers: Extreme values in the data (outliers)    can disproportionately influence the model's coefficients."
      ],
      "metadata": {
        "id": "ew0Oy_SifgtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "ANS:- Some alternatives to Logistic Regression for\n",
        "      classification tasks include:\n",
        "\n",
        "      1. Support Vector Machines (SVM): Effective in     \n",
        "         high-dimensional spaces and can model complex non-linear relationships using kernel tricks.  Good for relatively small to medium-sized datasets.\n",
        "\n",
        "      2. Decision Trees/Random Forests: Easy to interpret,\n",
        "         can handle both categorical and numerical data, and are less sensitive to outliers. Random Forests, an ensemble of decision trees, generally improve accuracy and robustness.\n",
        "\n",
        "      3. Naive Bayes:  Based on Bayes' theorem, assuming\n",
        "         feature independence.  Simple and fast, performs well with high-dimensional data and text classification.\n",
        "\n",
        "      4. K-Nearest Neighbors (KNN):  A simple instance-based\n",
        "         learning algorithm.  Predicts the class of a data point based on the classes of its k-nearest neighbors.  Can be computationally expensive for large datasets.\n",
        "\n",
        "      5. Neural Networks (Multilayer Perceptrons):  Powerful\n",
        "         models capable of learning complex patterns.  Can be computationally intensive to train and require careful tuning of hyperparameters.  Often outperform other methods when large datasets are available.\n",
        "\n",
        "      6. Gradient Boosting Machines (GBM) like XGBoost,\n",
        "         LightGBM, CatBoost: Ensemble methods that combine multiple weak learners (typically decision trees) to create a strong predictive model. Often provide state-of-the-art performance on various classification problems."
      ],
      "metadata": {
        "id": "U2aBpAicf7AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What are Classification Evaluation Metrics?\n",
        "\n",
        "ANS:-Classification Evaluation Metrics\n",
        "\n",
        "     Accuracy: The ratio of correctly classified instances to the total number of instances.  Simple to understand but can be misleading when dealing with imbalanced datasets.\n",
        "\n",
        "     Precision: Out of all the instances predicted as positive, what proportion was actually positive?  Focuses on the correctness of positive predictions.  High precision means few false positives.\n",
        "\n",
        "     Recall (Sensitivity or True Positive Rate): Out of all the actual positive instances, what proportion was correctly predicted as positive?  Focuses on identifying all positive instances. High recall means few false negatives.\n",
        "\n",
        "     F1-Score: The harmonic mean of precision and recall.  Provides a balanced measure of both precision and recall, especially useful when there is an uneven class distribution.\n",
        "\n",
        "     Specificity (True Negative Rate): Out of all the actual negative instances, what proportion was correctly predicted as negative?  Focuses on the correctness of negative predictions.  High specificity means few false positives.\n",
        "\n",
        "     ROC Curve (Receiver Operating Characteristic Curve): Plots the true positive rate (recall) against the false positive rate at various classification thresholds.  Visualizes the trade-off between sensitivity and specificity.\n",
        "\n",
        "     AUC (Area Under the Curve):  The area under the ROC curve.  A single number that summarizes the overall performance of a classifier across all possible thresholds.  Higher AUC indicates better performance (1 is a perfect classifier).\n",
        "\n",
        "     Log-Loss:  Measures the uncertainty of the classifier's predictions.  Lower log-loss indicates better performance.  Penalizes confident incorrect predictions more heavily.\n",
        "\n",
        "     Confusion Matrix: A table showing the counts of true positives, true negatives, false positives, and false negatives.  Provides a detailed breakdown of the classifier's performance across different classes."
      ],
      "metadata": {
        "id": "GbkLf1N5gVrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "ANS:- Class imbalance, where one class significantly\n",
        "      outnumbers others in a dataset, can severely affect the performance of Logistic Regression and other classification algorithms.\n",
        "\n",
        "     1. Biased Model:  A Logistic Regression model trained on\n",
        "        an imbalanced dataset will likely be biased towards the majority class.  Since the algorithm aims to minimize overall error, it might prioritize correctly classifying the abundant majority class at the expense of the minority class.  This results in a model that performs poorly on the minority class, even though it might achieve high overall accuracy.\n",
        "\n",
        "     2. Misleading Accuracy:  Accuracy, a commonly used\n",
        "        metric, can be misleading in the context of imbalanced datasets.  A model that always predicts the majority class could achieve a high accuracy if the majority class dominates the data.  However, such a model is useless for identifying instances of the minority class, which might be the more important task.\n",
        "\n",
        "     3. Poor Generalization: A model biased towards the\n",
        "        majority class will generalize poorly to new data, especially when the class distribution in the new data differs from that of the training data.\n",
        "\n",
        "     4. Decision Threshold Bias:  The default decision\n",
        "        threshold of 0.5 used to classify instances might not be optimal for imbalanced datasets.  Adjusting the threshold might improve the performance on the minority class, but this often involves a trade-off with the performance on the majority class.\n",
        "\n",
        "     5. Suboptimal Coefficients:  The estimated coefficients\n",
        "        in the logistic regression model might not accurately reflect the true relationships between the features and the target variable in the presence of class imbalance."
      ],
      "metadata": {
        "id": "kKhr7kjxguZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "ANS:- Key Hyperparameters in Logistic Regression:\n",
        "\n",
        "      1. Regularization Parameter (C or inverse of lambda):\n",
        "        - Controls the strength of regularization (L1 or L2).\n",
        "        - Smaller values of C correspond to stronger\n",
        "          regularization.\n",
        "        - Tuning C helps to prevent overfitting and improve\n",
        "          generalization.\n",
        "\n",
        "      2. Penalty (L1 or L2):\n",
        "        - Specifies the type of regularization:\n",
        "        - L1 (Lasso): Encourages sparsity in the coefficients\n",
        "          (some coefficients become zero).  Useful for feature selection.\n",
        "        - L2 (Ridge): Shrinks the coefficients towards zero,\n",
        "          but doesn't typically make them exactly zero.  Effective for multicollinearity.\n",
        "        - Choosing the right penalty depends on the data and  the goal of the model.\n",
        "\n",
        "      3. Solver:\n",
        "        - Specifies the algorithm used to optimize the\n",
        "          model's coefficients (e.g., 'liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga').\n",
        "        - Different solvers have varying computational costs\n",
        "          and performance characteristics.\n",
        "        - The choice of solver may depend on the size of the\n",
        "          dataset, the number of features, and the type of regularization used.\n",
        "\n",
        "      4. Maximum Iterations:\n",
        "        - The maximum number of iterations the solver will\n",
        "          run\n",
        "         to find the optimal coefficients.\n",
        "        - Increasing the maximum iterations might improve the\n",
        "         optimization, but takes longer.\n",
        "        - If the solver doesn't converge, increasing this may\n",
        "         help, but check for other issues as well.\n",
        "\n",
        "      5. Multi_class:\n",
        "         - How to handle multiple classes (for multi-class\n",
        "           classification).\n",
        "         - 'ovr' (one-vs-rest) or 'multinomial'."
      ],
      "metadata": {
        "id": "gtTJkvEWhazX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.C What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "ANS:- There are several solvers available for logistic\n",
        "      regression, each with its own strengths and weaknesses.  The choice of solver depends on factors such as the size of the dataset, the number of features, and the type of regularization used.\n",
        "\n",
        "      1. liblinear:\n",
        "        - Suitable for small to medium-sized datasets.\n",
        "        - Works well with L1 and L2 regularization.\n",
        "        - Often a good default choice for binary\n",
        "          classification.\n",
        "\n",
        "      2. newton-cg, lbfgs, sag, saga:\n",
        "        - Suitable for larger datasets.\n",
        "        - These are second-order methods (or approximations\n",
        "          thereof) and can converge faster than liblinear for larger datasets.\n",
        "        - They typically work with L2 regularization.\n",
        "        - 'newton-cg', 'lbfgs' and 'sag' don't handle L1\n",
        "           penalty.\n",
        "        - 'sag' and 'saga' are faster for large datasets.\n",
        "        - 'saga' also supports L1 penalty.\n",
        "\n",
        "        Which one to use?\n",
        "\n",
        "       - For small to medium datasets, liblinear is often a\n",
        "         good starting point, especially if L1 regularization is needed.\n",
        "       - For larger datasets, 'sag' or 'saga' are usually the\n",
        "         best choices due to their speed, especially for L2 regularization.  'saga' is also useful if you have L1 regularization.\n",
        "       - If convergence speed is critical and L2\n",
        "         regularization is appropriate, 'newton-cg' or 'lbfgs' are reasonable options."
      ],
      "metadata": {
        "id": "MtQBp_DSiLaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "ANS:- Logistic Regression is inherently designed for binary\n",
        "      classification (two classes). To extend it to multiclass classification (more than two classes), several strategies are commonly employed:\n",
        "\n",
        "      1. One-vs-Rest (OvR) or One-vs-All (OvA):\n",
        "\n",
        "      - Train a separate binary logistic regression model for\n",
        "        each class.\n",
        "      - For each model, one class is treated as the positive\n",
        "        class, and all other classes are combined into a single negative class.\n",
        "      - To classify a new instance, you run it through all\n",
        "        the models and choose the class with the highest predicted probability.\n",
        "\n",
        "     2. Multinomial Logistic Regression (Softmax Regression):\n",
        "\n",
        "     - This approach models the probabilities of all classes\n",
        "       simultaneously using a single model.\n",
        "     - Instead of using a sigmoid function, it uses a softmax\n",
        "       function to output probabilities for each class. The softmax function ensures that the predicted probabilities for all classes sum up to 1.\n",
        "     - It directly optimizes the likelihood of the correct\n",
        "       class given the input features.\n"
      ],
      "metadata": {
        "id": "FZMNdww2iuu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "ANS:- Advantages of Logistic Regression:\n",
        "      1. Simplicity and Interpretability: Logistic regression\n",
        "         is relatively simple to understand and interpret.  The coefficients of the model can be directly related to the input features, providing insight into the importance of each feature in predicting the outcome. This makes it easier to explain the model's predictions to stakeholders who may not have a technical background.\n",
        "      2. Efficiency:  Logistic regression is computationally\n",
        "         efficient, especially for smaller to medium-sized datasets.  Training and prediction times are generally fast, making it a good choice for situations where quick results are needed.\n",
        "      3. No Feature Scaling Required: Unlike some other\n",
        "         machine learning algorithms, logistic regression does not necessarily require feature scaling (standardization or normalization).  However, scaling can still be beneficial, especially if different features have vastly different ranges.\n",
        "      4. Probabilistic Output: Logistic regression outputs\n",
        "         probabilities, which can be very useful in applications where the uncertainty of the prediction is important. For example, in medical diagnosis, knowing the probability of a disease is more informative than a simple yes/no prediction.\n",
        "      5. Handles Multicollinearity: Logistic regression can   handle multicollinearity (high correlation between input features) to a reasonable extent, although regularization techniques (L1 or L2) are often used to mitigate its negative effects.\n",
        "      6. Works Well with Binary and Multiclass  \n",
        "        Classification:  While naturally suited for binary classification, logistic regression can be extended to multiclass problems using strategies like one-vs-rest or multinomial logistic regression.\n",
        "\n",
        "\n",
        "     Disadvantages of Logistic Regression:\n",
        "      1. Assumes Linearity of Independent Variables and   \n",
        "         Log-odds: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. This can be a limitation if the true relationship is non-linear.  In such cases, feature engineering or transformations may be required to introduce non-linearity.\n",
        "      2. Sensitive to Outliers: Logistic regression can be\n",
        "         sensitive to outliers, which can disproportionately influence the model's coefficients and predictions. Outlier detection and handling techniques are important when working with logistic regression.\n",
        "      3. Prone to Overfitting:  With many features or limited\n",
        "         data, logistic regression can overfit the training data, leading to poor generalization performance on unseen data. Regularization techniques help to alleviate this problem.\n",
        "      4. Requires Independent Observations: Logistic\n",
        "         regression assumes that the observations in the dataset are independent of each other.  Violation of this assumption can affect the reliability of the model.\n",
        "       5. May Not Perform Well with Complex Relationships:\n",
        "          For highly complex datasets or non-linear relationships between features and the target variable, other more powerful algorithms like neural networks or support vector machines may be more appropriate."
      ],
      "metadata": {
        "id": "a12-QTUkjStN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. What are some use cases of Logistic Regression?\n",
        "\n",
        "ANS:- Some use cases of Logistic Regression:\n",
        "\n",
        "      1. Credit Scoring: Predict the likelihood of a loan\n",
        "         applicant defaulting on a loan.\n",
        "      2. Medical Diagnosis: Diagnose diseases based on\n",
        "         patient symptoms and medical history.\n",
        "      3. Customer Churn Prediction: Predict whether a\n",
        "         customer is likely to cancel their subscription.\n",
        "      4. Spam Detection: Classify emails as spam or not spam.\n",
        "      5. Image Classification: Classify images into different\n",
        "         categories (e.g., cat vs. dog).\n",
        "      6. Fraud Detection: Detect fraudulent transactions.\n",
        "      7. Risk Assessment: Assess the risk of an event\n",
        "         occurring (e.g., car accidents).\n",
        "      8. Marketing: Predict customer responses to marketing\n",
        "         campaigns.\n",
        "      9. Customer Segmentation: Group customers into\n",
        "         different segments based on their characteristics and behavior.\n",
        "      10. Sentiment Analysis: Analyze text to determine the\n",
        "          sentiment (positive, negative, or neutral) expressed in it.\n"
      ],
      "metadata": {
        "id": "xgQCYClMj0Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "ANS:- Logistic Regression:\n",
        "      - Used for binary classification (two classes).\n",
        "      - Outputs the probability of an instance belonging to\n",
        "        one of the two classes.\n",
        "      - Uses a sigmoid function to map the linear combination\n",
        "        of input features to a probability between 0 and 1.\n",
        "\n",
        "      Softmax Regression:\n",
        "      - An extension of logistic regression for multiclass\n",
        "        classification (more than two classes).\n",
        "      - Outputs a probability distribution over all possible\n",
        "        classes for each instance.\n",
        "      - Uses the softmax function to convert the linear\n",
        "        combinations of input features into a probability distribution.  The probabilities for all classes sum to 1.\n",
        "      - Effectively performs one-vs-all classification but    within a single model."
      ],
      "metadata": {
        "id": "Sf8V7rzSkMCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "ANS:- The choice between One-vs-Rest (OvR) and Softmax for\n",
        "      multiclass classification depends on several factors:\n",
        "\n",
        "      1. Dataset Size:\n",
        "       - For smaller datasets, OvR might be computationally\n",
        "         less expensive because it trains multiple binary classifiers, which are usually faster to train than a single multinomial classifier (Softmax).\n",
        "       - For larger datasets, the computational cost\n",
        "         difference may be less significant, and Softmax often provides better performance due to its simultaneous consideration of all classes.\n",
        "\n",
        "     2. Class Relationships:\n",
        "       - OvR treats each class independently, assuming no\n",
        "        relationship between classes. If there are inherent relationships between your classes, Softmax might be a better choice, as it models these relationships explicitly.\n",
        "      - If classes are largely independent or if the\n",
        "        relationship between them is not crucial for classification, OvR is a suitable approach.\n",
        "\n",
        "     3. Performance:\n",
        "       - Softmax often achieves higher accuracy than OvR,\n",
        "         particularly when there are complex relationships between the classes.\n",
        "       - OvR can sometimes be simpler to interpret due to the\n",
        "         individual binary classifiers.\n",
        "\n",
        "     4. Computational Resources:\n",
        "       - Softmax generally requires more computational\n",
        "         resources (memory and processing power) to train, especially with a large number of classes.\n"
      ],
      "metadata": {
        "id": "IVjoRFQQkjBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "ANS:- In logistic regression, coefficients represent the\n",
        "      change in the log-odds of the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.\n",
        "\n",
        "      To interpret the coefficients:\n",
        "\n",
        "     1. Log-Odds:  The coefficient's value is directly        related to the log-odds. A positive coefficient indicates that an increase in the predictor variable increases the log-odds of the positive outcome (e.g., the probability of an event occurring).  A negative coefficient suggests the opposite: an increase in the predictor decreases the log-odds of a positive outcome.\n",
        "\n",
        "     2. Odds Ratio:  To get a more intuitive understanding, exponentiate the coefficient (e^coefficient) to obtain the odds ratio.  The odds ratio represents the change in the odds of the outcome for a one-unit change in the predictor variable.\n",
        "\n",
        "     Odds Ratio > 1: An increase in the predictor variable is associated with an increase in the odds of the positive outcome.  The larger the odds ratio, the stronger the association.\n",
        "     Odds Ratio < 1: An increase in the predictor variable is associated with a decrease in the odds of the positive outcome.  The smaller the odds ratio, the stronger the association.\n",
        "     Odds Ratio = 1:  There is no association between the predictor variable and the outcome.\n"
      ],
      "metadata": {
        "id": "pijA-yHek_zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL**"
      ],
      "metadata": {
        "id": "Cpmx0Ql7levF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "RsDkm8-HliEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = (iris.target == 0).astype(int)  # Binary classification: is the class 'setosa'?\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFYioFytmL3Y",
        "outputId": "f925135a-995f-42e2-a928-b8b6f949ef1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy."
      ],
      "metadata": {
        "id": "V9s166GQmWQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = (iris.target == 0).astype(int)  # Binary classification: is the class 'setosa'?\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear') # Use liblinear solver for L1\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with L1 Regularization Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiYTmwSOmfnD",
        "outputId": "4089eb7d-68b8-44eb-8173-fcffa572e4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L1 Regularization Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "baaJ-ueSmmsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = (iris.target == 0).astype(int)  # Binary classification: is the class 'setosa'?\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')  # Use liblinear solver for L2\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with L2 Regularization Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Coefficients:\")\n",
        "model.coef_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzu31fnJmyTi",
        "outputId": "115fe833-f372-46f6-d984-f56195a672bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L2 Regularization Accuracy: 1.00\n",
            "Coefficients:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.3711229 ,  1.409712  , -2.15210117, -0.95474179]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "VMgBi20pm0mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = (iris.target == 0).astype(int)  # Binary classification: is the class 'setosa'?\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model with Elastic Net regularization\n",
        "# Note: 'elasticnet' penalty requires the 'saga' solver\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with Elastic Net Regularization Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Coefficients:\")\n",
        "model.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UObGrmtmm9qu",
        "outputId": "49d4bf95-2088-4c88-a0d8-ec9a434cd964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with Elastic Net Regularization Accuracy: 1.00\n",
            "Coefficients:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.25780503,  1.55380874, -2.30817529, -0.74457283]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'."
      ],
      "metadata": {
        "id": "DYrtL1gLnAX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model with multi_class='ovr'\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear') # Use liblinear for ovr\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression (ovr): {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55YgvwFOnIE9",
        "outputId": "0d9e3cd2-cd8f-45d3-9e41-e1fa03b09568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression (ovr): 0.9777777777777777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "DfQ_eyJznJvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear') # Use liblinear for L1 and L2\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5) # 5-fold cross-validation\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Accuracy of Logistic Regression (GridSearchCV): {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ_dNC96nTnA",
        "outputId": "f61aa45c-54f1-4ad0-f037-acc146b39bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Accuracy of Logistic Regression (GridSearchCV): 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "QzmKVqmenVpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # 5-fold cross-validation\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "avg_accuracy = np.mean(scores)\n",
        "print(f\"Average Accuracy (Stratified K-Fold): {avg_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXeodHHVndSP",
        "outputId": "388413da-96ee-46fb-8bca-9dba3164442a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy (Stratified K-Fold): 0.9667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "clFTcn8wne8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "file_path = 'your_file.csv'\n",
        "\n",
        "try:\n",
        "    # Load the dataset from the CSV file\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Assuming the last column is the target variable\n",
        "    X = data.iloc[:, :-1]  # Features\n",
        "    y = data.iloc[:, -1]    # Target variable\n",
        "\n",
        "    # Handle missing values (if any) - replace with appropriate strategy\n",
        "    X.fillna(X.mean(), inplace=True) # Example: fill with mean\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create and train the logistic regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy of Logistic Regression: {accuracy}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k84fVjdVnmXE",
        "outputId": "e187b9b4-22bc-4237-cb1f-881c51b429ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at 'your_file.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "_c1BVVa8npzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Load the Iris dataset (assuming it's already loaded as in the previous examples)\n",
        "# ... (your code to load the dataset) ...\n",
        "\n",
        "# Define the parameter distributions for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.001, scale=100),  # Continuous uniform distribution for C\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']  # Include all relevant solvers\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=5,       # 5-fold cross-validation\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all processors\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Accuracy of Logistic Regression (RandomizedSearchCV): {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0foPiyln3Dr",
        "outputId": "f843ee2f-1130-4bdf-a140-4dd0dcf2e4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(30.42522429595377), 'penalty': 'l2', 'solver': 'saga'}\n",
            "Accuracy of Logistic Regression (RandomizedSearchCV): 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "40 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.94285714        nan        nan        nan 0.95238095        nan\n",
            " 0.96190476 0.94285714 0.96190476        nan 0.94285714        nan\n",
            " 0.96190476 0.95238095 0.96190476        nan 0.94285714 0.96190476\n",
            " 0.94285714        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "pkD8e6Xcn7bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "base_model = LogisticRegression(solver='liblinear') # Choose an appropriate solver\n",
        "\n",
        "# Create a OneVsOneClassifier with the Logistic Regression model as the base estimator\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train the OvO model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of One-vs-One Logistic Regression: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I7GZafXoOLk",
        "outputId": "e66049e1-cb84-4b8c-d9d3-e54e0569d602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of One-vs-One Logistic Regression: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "1c0QrHDooQvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oadWp-hbovfs",
        "outputId": "e713fc98-fdf7-44ce-bfae-3dafec2fb76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "z9_jdmfyo1za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have already trained a Logistic Regression model (e.g., 'model')\n",
        "# and have predictions ('y_pred') and true labels ('y_test') as in the previous examples.\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijaLoaqao_Sc",
        "outputId": "4c5a52a9-9a5e-408b-fffa-318df58ff92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.95\n",
            "Recall: 0.99\n",
            "F1-Score: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "bheWZpMKpA-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the imbalanced dataset (example using make_classification)\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5,\n",
        "                           n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                           flip_y=0, random_state=42)\n",
        "\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Train a Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight=class_weight_dict)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model (example using accuracy and f1-score)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogrhojyQpKjI",
        "outputId": "415a3578-7e3f-4a64-8767-e314fbdece07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9550\n",
            "F1-score: 0.7907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "sIWd9wyXpMNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Drop rows with missing target\n",
        "titanic = titanic.dropna(subset=['survived'])\n",
        "\n",
        "# Fill missing values\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "X = titanic[features]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Convert categorical variables\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "kvtiZL33pnjy",
        "outputId": "75f05752-37da-45f0-97b6-17439a3d6fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-9a907306142a>:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-1-9a907306142a>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.81\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "GpjMLemSpsN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling}\")\n",
        "\n",
        "# Feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_scaling = LogisticRegression()\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaling}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke67ClwTp14u",
        "outputId": "38fe5003-e5ef-476f-bb27-220e85ed52b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0\n",
            "Accuracy with scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "M7cReD-Mp3ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBuc292NqG8g",
        "outputId": "b75dc6f0-ecff-4422-ea41-fc62b62d772b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "r3LHzGPRqIdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model with a custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5)  # C is the inverse of regularization strength\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_ETJVrUqSKy",
        "outputId": "93de9933-9cd0-42b0-f480-db7130418f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "l8UfMPZWqUYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = {'Feature1': [1, 2, 3, 4, 5],\n",
        "        'Feature2': [6, 7, 8, 9, 10],\n",
        "        'Feature3': [11, 12, 13, 14, 15],\n",
        "        'Target': [0, 1, 0, 1, 0]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
        "y = df['Target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get coefficients\n",
        "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_[0]})\n",
        "print(coefficients)\n",
        "\n",
        "# Identify important features (e.g., based on absolute coefficient magnitude)\n",
        "coefficients['Abs_Coefficient'] = abs(coefficients['Coefficient'])\n",
        "important_features = coefficients.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "print(\"\\nImportant Features:\")\n",
        "important_features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "78LCE4KMqhnY",
        "outputId": "716c0b65-8edb-49e2-e724-b9534e972d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Feature  Coefficient\n",
            "0  Feature1     0.140681\n",
            "1  Feature2     0.140683\n",
            "2  Feature3     0.140685\n",
            "\n",
            "Important Features:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Feature  Coefficient  Abs_Coefficient\n",
              "2  Feature3     0.140685         0.140685\n",
              "1  Feature2     0.140683         0.140683\n",
              "0  Feature1     0.140681         0.140681"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9086674-0fdc-4b2f-9f8d-a28043cfc259\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Coefficient</th>\n",
              "      <th>Abs_Coefficient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Feature3</td>\n",
              "      <td>0.140685</td>\n",
              "      <td>0.140685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Feature2</td>\n",
              "      <td>0.140683</td>\n",
              "      <td>0.140683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Feature1</td>\n",
              "      <td>0.140681</td>\n",
              "      <td>0.140681</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9086674-0fdc-4b2f-9f8d-a28043cfc259')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9086674-0fdc-4b2f-9f8d-a28043cfc259 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9086674-0fdc-4b2f-9f8d-a28043cfc259');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ad4e20a3-f6fb-4563-b714-90245c56967a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ad4e20a3-f6fb-4563-b714-90245c56967a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ad4e20a3-f6fb-4563-b714-90245c56967a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d1796288-2abb-48a1-a1ab-f280dc5ff9c0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('important_features')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d1796288-2abb-48a1-a1ab-f280dc5ff9c0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('important_features');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "important_features",
              "summary": "{\n  \"name\": \"important_features\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Feature3\",\n          \"Feature2\",\n          \"Feature1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Coefficient\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.3046856544306493e-06,\n        \"min\": 0.14068060918064368,\n        \"max\": 0.14068521855195254,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.14068521855195254,\n          0.14068291386628962,\n          0.14068060918064368\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abs_Coefficient\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.3046856544306493e-06,\n        \"min\": 0.14068060918064368,\n        \"max\": 0.14068521855195254,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.14068521855195254,\n          0.14068291386628962,\n          0.14068060918064368\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "2_VHvg84qjzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Drop rows where target is missing\n",
        "titanic = titanic.dropna(subset=[\"survived\"])\n",
        "\n",
        "# Fill missing values\n",
        "titanic[\"age\"].fillna(titanic[\"age\"].median(), inplace=True)\n",
        "titanic[\"embarked\"].fillna(titanic[\"embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "features = [\"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\", \"embarked\"]\n",
        "X = titanic[features]\n",
        "y = titanic[\"survived\"]\n",
        "\n",
        "# Encode categorical variables\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ey51a3Eq4Vs",
        "outputId": "891eae27-ba67-432d-a362-69c742bd3598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-78147b2fe33f>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic[\"age\"].fillna(titanic[\"age\"].median(), inplace=True)\n",
            "<ipython-input-14-78147b2fe33f>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic[\"embarked\"].fillna(titanic[\"embarked\"].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio."
      ],
      "metadata": {
        "id": "8jZNGl9ZrFXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have already trained a Logistic Regression model (e.g., 'model')\n",
        "# and have predictions ('y_pred') and true labels ('y_test') as in the previous examples.\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Calculate average precision score\n",
        "average_precision = average_precision_score(y_test, y_probs)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "i8OaF2ngrSPw",
        "outputId": "6954c17c-27d5-4a5a-d6f5-13c3a815c4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "A-78eLx0rT8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "for solver in solvers:\n",
        "    # Train a Logistic Regression model with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)  # Increased max_iter\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with solver '{solver}': {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izmxbBMKrdLW",
        "outputId": "16980027-3c11-463f-d367-aa983c3b816c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with solver 'liblinear': 0.9778\n",
            "Accuracy with solver 'saga': 1.0000\n",
            "Accuracy with solver 'lbfgs': 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "y87W3IQGrfbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Assuming you have already trained a Logistic Regression model (e.g., 'model')\n",
        "# and have predictions ('y_pred') and true labels ('y_test') as in the previous examples.\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFtETjabrnZ3",
        "outputId": "4bfda253-fda9-48c1-cadc-ebff004f6cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "DO5jeRIIro4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling}\")\n",
        "\n",
        "# Feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_scaling = LogisticRegression()\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaling}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGVnkFSQrzTp",
        "outputId": "72825bc7-b68d-4cf8-b86b-486105b6fbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0\n",
            "Accuracy with scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation."
      ],
      "metadata": {
        "id": "mPjXVKIpr1An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Assuming X and y are your features and target variable\n",
        "# ... (load your data and define X and y) ...\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Create a LogisticRegression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)  # 5-fold cross-validation\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Get the best C value\n",
        "best_c = grid_search.best_params_['C']\n",
        "print(f\"Optimal C: {best_c}\")\n",
        "\n",
        "# Train the model with the best C\n",
        "best_model = LogisticRegression(C=best_c)\n",
        "best_model.fit(X,y)\n",
        "\n",
        "# You can then use best_model for predictions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QRgDuR2xr-3E",
        "outputId": "e2226932-dff8-41cc-f491-20024e655a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=10)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "TPRpjBGfsAsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model, filename)\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(filename)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# You can now evaluate the loaded model's performance\n",
        "# (e.g., using accuracy_score)\n",
        "# ... (your code to evaluate the model) ..."
      ],
      "metadata": {
        "id": "Njgs1zpYsVOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}